
experiment: 1: 1_Tanh vs 1_Linear, dataset_files_subsampled_dense_np2
    Goal: compare decoder's Linear vs Tanh as well as 1,2,8 layers
    Observations:
        Not much difference for Linear vs Tanh
        1, 2 layers converge so fast, 8 layers takes forever to train
            20 epochs
        Shuffle seems to work better

experiment: 2: 2_TanhReLU vs 2_LinearReLU, dataset_files_subsampled_dense_np2, 20 epochs
    Goal: compare encoder's ReLU to the best performing of experiment 1
        Best from experiment 1: shuffle true, 1 or 2 layers

    Observations
        ReLU works better than Tanh for encoder, for both TanhReLU and LinearReLU
        adam and optimrmsprop work both as well
        ReLU encoder and Linear decoder work better than just Linear decoder
        TanhRelU and LinearReLU both seem to work well, so just stick with LinearReLU
        2 layers seems to work better than 1 layer
        0.95 the best, then 0.99

experiment: 3: 3_TanhLinearvs 3_LinearLinear, dataset_files_subsampled_dense_np2, 20 epochs
    Goal: compare encoder's ReLU to the best performing of experiment 2
        TanhLinear vs LinearLinear
        shuffle true, 2 layers, both adam and optimrmsprop, 0.95, 0.99, 1

    Observations: a Linear encoder is generally worse than a ReLU encoder
        LinearLinear better than TanhLinear, but still worse than ReLU
        Clearly ReLU is the best for the encoder, but Tanh and Linear work for
        decoder

experiment: 4: MSECriterion vs SmoothL1Criterion
    Goal: compare SmoothL1Criterion vs best in experiment3
    Run: 4_SL1TanhReLU 4_SL1LinearReLU
        0.99,0.95,1 all work similarly, so just use 0.99

    Comment: may try to do LinearLinear again

    Observations:
        SL1 blows everything away; so good across the board (but it could just be because SL1 loss is incomparable with MSE though)
            Works best with optimrmsprop (and corresponding 0.001)
            TanhReLU and LinearReLU equal


experiment: 5: Selected Criterion
    Goal: compare a Criterion for all elements rather than for selected elements
        5_Sl1BCETanhReLU: Tanh will be for state vector, Sigmoid for obj_attr
        5_Sl1BCELinearReLU: Linear will be for state vector, Sigmoid for obj_attr
            For some reason Linear gives me an error
    Note: Need to visualize results from 2, 4, 5 to evaluate performance of 5

    Observations:
        optimrmsprop works better than adam generally here
        Splitted criterion is the correct way to go.
        Does not do obj-obj after visualization. Perhaps this problem is way too hard to learn.
        Try doing relative positioning. Then try less in less out

-- NOW YOU SHOULD VISUALIZE THE OUTPUT  -- none of them can do object-object

experiment: 6: Relative Coordinates
    Goal: compare 5_SL1BCELinearReLU vs 6_SL1BCELinearReLURel with relative coords
    Also tested different rnn_dims, and it seems like it doesn't matter too much.
        rnn dim of 256 and 512, 1 and 2 layers

    Observations:
        seems like relative coordinates converge much faster
        seems like relative coordinates has better convergence and training
        but only for some configurations

    Starting Now: I will use optimrmsprop, SL1BCELinearReLURel, lr=0.001, 2 layers, dim 128, relative

-- TEST ON THE TRAINING SET FIRST! YOU HAVE TO TEST ON THE TRAINING SET! AND GENERATE VIDEOS
        -- note that 6_SL1BCELinearReLURel_opt_optimrmsprop_lr_0.001_predict.out is tested on train set
        -- seems like the test error is actually representative of the training error
            so how well it does on test reflects how well it does on train?

experiment 7: only worldm1_np=2_ng=0: 48000 examples

experiment 8: np2_ng=0: 192,000 examples

experiment 9: np2_ng=0 (dataset 4): 1in1out

experiment 10: dataset 4, feedforward, 10 in 10 out, ff_runner.py, run on the cpu

experiment 11: dataset 3, feedforward, 1 in 1 out, ff_runner.py, run on the cpu
    let's try to simulate on this

experiment 11b: dataset 3, feedforward, 10 in 10 out, ff_runner.py, run on the cpu

experiment 11c: dataset 3, feedforward, 10 in 10 out, ff_runner.py, run on the cpu, Sigmoid in FF, 50 epcs, lr=0.99, bsize=80

experiment 11d: dataset 3, feedforward, 10 in 1 out, runner.py, run on the cpu, SoftSign in FF, 50 epcs, lr=0.95, bsize=80

experiment 12: priority sampling (no sharpening), dataset 3, 10 in 10 out, bsize 30
    12b: sharpening with exponent 2
    12c: different learning rates (0.9 and 0.97), no sharpening, start at 1 epoch, 1 or 2 layers
        Killed
            12c_opt_optimrmsprop_layers_1_rnn_dim_256_lr_0.005_lrdecay_0.97.out
            12c_opt_optimrmsprop_layers_1_rnn_dim_256_lr_0.005_lrdecay_0.9.out
    12d: 2 in 1 out, 0.9 lr, no sharp, start at 1 epoch, 1 or 2 layers

    Observations
        2 layers is the way to go, 0.001 is good

experiment 13: priority sampling, dataset 3, 10 in 10 out, bsize 60, velocity only
    lrdeacy 0.90, lr [1e-4,5e-4,1e-3], 1, 2, 3 layers

    Observations
        killed the 1e-4 jobs, killed 5e-4 jobs, killed 1e-3 jobs
        also killed jobs with 3e-4 + lr=0.99

    Converged:
        13_layers_1_sharpen_1_lr_0.005
        13_layers_1_sharpen_2_lr_0.005
        13_layers_2_sharpen_1_lr_0.005
        13_layers_2_sharpen_2_lr_0.005
        13_layers_3_sharpen_1_lr_0.005
        13_layers_3_sharpen_2_lr_0.005
        13_layers_1_sharpen_1_lr_0.01
        13_layers_1_sharpen_2_lr_0.01
        13_layers_2_sharpen_1_lr_0.01
        13_layers_2_sharpen_2_lr_0.01
        13_layers_3_sharpen_1_lr_0.01
        13_layers_3_sharpen_2_lr_0.01

experiment 14: experiment 13, but with sigmoid

experiment: 7: 8_1i1o
    Goal: comapre the best from experiment 7 with 1 in 1 out
        Dataset: /om/data/public/mbchang/physics-data/2
        Batch_size: 65
        Take the 1 in 1 out from the middle for training. or actually it can be
        1 in "19" out for simulation
    Observations: lr should be less than 0.01

    Evaluate by actual performance

experiment: 7
    Goal: compare the best from experiment 6 with augmented permutations of examples
        or bidirectional LSTM: something that tells the model to not care about
        order


experiment: 8 Huge Feedforward baseline
    Goal: compare



NOTE
    - predpos[0,i+1,:] = predpos[0,i,:] + vel[0,i+1,:]  # so this method is not completely correct because of subsampling.
        - for now you can just use this dataset to debug, but for the real thing you need to regenerate data.
        - note that you should do a threshold that sets velocities to 0 if they are less than e-10
            (you should check numpy's precision though)
